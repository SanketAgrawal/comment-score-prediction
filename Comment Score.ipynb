{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T08:26:19.108663Z",
     "start_time": "2019-04-14T08:26:19.079108Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:37.829298Z",
     "start_time": "2019-04-14T06:59:37.636443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset=pd.read_csv('./dataset/train.csv')\n",
    "dataset1=dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:37.900710Z",
     "start_time": "2019-04-14T06:59:37.831003Z"
    }
   },
   "outputs": [],
   "source": [
    "#Sample instance of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:38.221696Z",
     "start_time": "2019-04-14T06:59:37.904439Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get column Names\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:38.559967Z",
     "start_time": "2019-04-14T06:59:38.224569Z"
    }
   },
   "outputs": [],
   "source": [
    "#Dataset Shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:38.916567Z",
     "start_time": "2019-04-14T06:59:38.567584Z"
    }
   },
   "outputs": [],
   "source": [
    "#Stats of the target variable\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.107515Z",
     "start_time": "2019-04-14T06:59:38.919016Z"
    }
   },
   "outputs": [],
   "source": [
    "#Outlier detection using box plot\n",
    "dataset.score.plot.box()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.132794Z",
     "start_time": "2019-04-14T06:59:39.110925Z"
    }
   },
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.319144Z",
     "start_time": "2019-04-14T06:59:39.137140Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "print(dataset.shape)\n",
    "columns=dataset.columns\n",
    "q25, q75=dataset.iloc[:,-1].quantile(0.25), dataset.iloc[:,-1].quantile(0.75)\n",
    "iqr = q75 - q25\n",
    "min = q25 - (iqr*2.0)\n",
    "max = q75 + (iqr*2.0)\n",
    "dataset = dataset.drop(dataset[dataset.iloc[:,-1] < min].index)\n",
    "dataset = dataset.drop(dataset[dataset.iloc[:,-1] > max].index)\n",
    "print(\"Outliers removed=\",dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.367997Z",
     "start_time": "2019-04-14T06:59:39.321348Z"
    }
   },
   "outputs": [],
   "source": [
    "#New stats of the dataset. These are used for initializing the weights of the NN\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.484618Z",
     "start_time": "2019-04-14T06:59:39.370376Z"
    }
   },
   "outputs": [],
   "source": [
    "#Take sufficient number of instances\n",
    "dataset=dataset.iloc[:39900,:]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T06:59:39.834148Z",
     "start_time": "2019-04-14T06:59:39.493490Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "stop_words=set(stopwords.words('english_for_lstm'))\n",
    "\n",
    "\"\"\"Stem words in list of tokenized words\"\"\"\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "\"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:04:16.538440Z",
     "start_time": "2019-04-14T06:59:39.841201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Append the parent comment and reply and then clean them.\n",
    "for i in range(dataset.shape[0]):\n",
    "#     line=re.sub('[^a-zA-Z0-9 ]*','',(dataset.iloc[i,3]+\" \"+dataset.iloc[i,1]).lower())\n",
    "    line=re.sub('[^a-zA-Z0-9 ]*','',dataset.iloc[i,1].lower())\n",
    "    new_line=[]\n",
    "    for word in line.split():\n",
    "        if word not in stop_words:\n",
    "            new_line.append(lemmatizer.lemmatize(word))\n",
    "    dataset.iloc[i,1]=' '.join(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:04:18.086087Z",
     "start_time": "2019-04-14T07:04:16.540872Z"
    }
   },
   "outputs": [],
   "source": [
    "#Tokenize and pad the sentences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(dataset.iloc[:,1])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"Vocab size=\",vocab_size)\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(dataset.iloc[:,1])\n",
    "\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs.shape)\n",
    "\n",
    "words=t.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:05:57.681257Z",
     "start_time": "2019-04-14T07:04:18.087990Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the embedding matrix\n",
    "embeddings_index = dict()\n",
    "f = open('/home/sanket/nltk_data/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.strip().lower().split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        if word in words:\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:05:57.694781Z",
     "start_time": "2019-04-14T07:05:57.684601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to generate the input matrix for NN\n",
    "def get_input(x):\n",
    "    x=np.reshape(x,[-1,max_length])\n",
    "    sp=tuple(list(x.shape)+[300])\n",
    "    mat = zeros(sp)\n",
    "    for k in range(sp[0]):\n",
    "        j=0\n",
    "        for i in x[k]:\n",
    "            mat[k][j]=embedding_matrix[i]\n",
    "            j+=1\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the NN Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:06:49.497307Z",
     "start_time": "2019-04-14T07:05:57.698930Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.0001\n",
    "training_steps = 50\n",
    "batch_size = 50\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "num_input =  300\n",
    "timesteps=max_length\n",
    "num_hidden_layer_1 = 1024\n",
    "num_hidden_layer_2 = 512\n",
    "num_hidden_layer_3=256\n",
    "num_classes =  1\n",
    "\n",
    "# tf Graph input\n",
    "X_tensor = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y_tensor = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'layer3': tf.Variable(tf.random.normal([num_hidden_layer_2, num_hidden_layer_3]),name=\"layer3_weight\"),\n",
    "    'out': tf.Variable(tf.random.normal([num_hidden_layer_3, num_classes],mean=2,stddev=3),name=\"out_weight\")\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random.normal([num_classes]),name=\"out_bias\"),\n",
    "    'layer3':tf.Variable(tf.random.normal([num_hidden_layer_3]),name=\"layer3_bias\")\n",
    "}\n",
    "\n",
    "\n",
    "def Model(x, weights, biases):\n",
    "\n",
    "    print(x.shape)\n",
    "    x=tf.unstack(x,timesteps,1)\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden_layer_1, forget_bias=0.2,reuse=tf.AUTO_REUSE)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden_layer_1, forget_bias=0.2,reuse=tf.AUTO_REUSE)\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden_layer_2, forget_bias=0)\n",
    "    \n",
    "    # Get lstm cell output\n",
    "    outputs, states_f, states_b = rnn.static_bidirectional_rnn(cell_fw=lstm_fw_cell,\n",
    "                                                   cell_bw=lstm_bw_cell,\n",
    "                                                   inputs=x,\n",
    "                                                   dtype=tf.float32)\n",
    "    print(len(outputs), type(outputs)) \n",
    "    output, states=rnn.static_rnn(lstm_cell, outputs, dtype=tf.float32)\n",
    "    output=tf.nn.dropout(output,keep_prob=0.7)\n",
    "    print(output, type(output))\n",
    "    outputs=tf.matmul(output[-1], weights['layer3']) + biases['layer3']\n",
    "    print(outputs, type(outputs)) \n",
    "    outputs=tf.nn.leaky_relu(outputs,alpha=0.9)\n",
    "    print(outputs, type(outputs)) \n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']\n",
    "\n",
    "logits = Model(X_tensor, weights, biases)\n",
    "prediction =logits\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.sqrt(tf.losses.mean_squared_error(prediction,Y_tensor))\n",
    "print(loss_op)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,epsilon=0.1)\n",
    "\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:06:49.540688Z",
     "start_time": "2019-04-14T07:06:49.498824Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split the data into train and validation set\n",
    "X_train, X_validation, Y_train, Y_Validation=train_test_split(padded_docs,dataset.score, test_size=0.3, random_state=50)\n",
    "print(np.array(X_train).shape,np.array(Y_train).shape,np.array(X_validation).shape,np.array(Y_Validation).shape)\n",
    "train_loss=[]\n",
    "valid_loss=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T07:06:49.643236Z",
     "start_time": "2019-04-14T07:06:49.542405Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate the training and validation error\n",
    "def cal_error():\n",
    "    print('Training Error:')\n",
    "    l=len(X_train)//batch_size\n",
    "    for i in range(0,len(train_loss),l):\n",
    "        print(sum(train_loss[i:i+l])/l)\n",
    "    print('\\nValidation Errors:')\n",
    "    l=len(X_validation)//batch_size\n",
    "    for i in range(0,len(valid_loss),l):\n",
    "        print(sum(valid_loss[i:i+l])/l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:15:02.249707Z",
     "start_time": "2019-04-07T17:34:55.393962Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True, device_count = {'CPU': 3})\n",
    "sess= tf.Session(config=config)\n",
    "pre_loss=10000000000\n",
    "# Run the initializer\n",
    "# sess.run(init)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'./checkpooint_01_04_19_d.ckpt')\n",
    "\n",
    "len_validation_set=len(X_validation)\n",
    "\n",
    "for step in range(training_steps):\n",
    "    print(\"steps=\",step)\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "        batch_x, batch_y = get_input(X_train[i:i+batch_size]), Y_train[i:i+batch_size]\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        batch_y=np.array(batch_y).reshape((-1,1))\n",
    "        j=i%len_validation_set\n",
    "        batch_x_val, batch_y_val = get_input(X_validation[j:j+batch_size]), Y_Validation[j:j+batch_size]\n",
    "        batch_x_val = batch_x_val.reshape((batch_size, timesteps, num_input))\n",
    "        batch_y_val=np.array(batch_y_val).reshape((-1,1))\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X_tensor: batch_x, \n",
    "                                      Y_tensor: batch_y})\n",
    "        if i % display_step== 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(loss_op, feed_dict={X_tensor: batch_x,\n",
    "                                                                 Y_tensor: batch_y})\n",
    "            val_loss = sess.run(loss_op, feed_dict={X_tensor: batch_x_val,\n",
    "                                                                 Y_tensor: batch_y_val})\n",
    "            if pre_loss>loss or i%1000==0:\n",
    "                pre_loss=min(loss,pre_loss)\n",
    "                saver.save(sess,'./checkpooint_01_04_19_e.ckpt')\n",
    "            train_loss.append(loss)\n",
    "            valid_loss.append(val_loss)\n",
    "\n",
    "            if i%1500==0:\n",
    "                print(\"batch=\",i)\n",
    "                \n",
    "                plt.plot(train_loss)\n",
    "                plt.show()\n",
    "                \n",
    "                plt.plot(valid_loss)\n",
    "                plt.show()\n",
    "                \n",
    "                cal_error()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-08T14:15:33.981846Z",
     "start_time": "2019-04-08T14:15:32.387116Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save the training and validation loss in file\n",
    "np.savetxt('train_loss',np.array(train_loss))\n",
    "np.savetxt('validation_loss',np.array(valid_loss))\n",
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T08:37:25.786959Z",
     "start_time": "2019-04-14T08:37:20.413551Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a session and restore the model weights\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True, device_count = {'CPU': 3})\n",
    "sess= tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'./checkpooint_01_04_19_e.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-14T08:37:24.840Z"
    }
   },
   "outputs": [],
   "source": [
    "batch=200\n",
    "for p in range(0,len(dataset1),batch):\n",
    "    batch_x, batch_y = get_input(padded_docs[p:p+batch]),dataset.score[p:p+batch]\n",
    "    batch_x = batch_x.reshape((batch, timesteps, num_input))\n",
    "    batch_y=batch_y\n",
    "    a=sess.run(prediction, feed_dict={X_tensor: batch_x})\n",
    "    b=sess.run(tf.math.round(a))\n",
    "#     print(p,P+batch)\n",
    "    dataset1.iloc[p:p+batch,0]=b\n",
    "#     for t in zip(b,batch_y,dataset1.iloc[p:p+batch,1],dataset1.iloc[p:p+batch,3]):\n",
    "#         print(t)\n",
    "dataset1.to_csv('results.csv',index=False)\n",
    "print(\"File Created!!!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations Used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Configuration 1:\n",
    "epsilon=default\n",
    "drop out=0.9\n",
    "forget gate value=0.5\n",
    "alpha leaky relu=0.7\n",
    "epochs=4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Configuration 2:\n",
    "epsilon=0.01\n",
    "drop out=0.9\n",
    "forget gate value=0\n",
    "alphs leaky relu=0.7\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Configuration 3:\n",
    "epsilon=0.1\n",
    "drop out=0.7\n",
    "forget gate value=0.2\n",
    "alphs leaky relu=0.9\n",
    "epochs=2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loss function chaned to RMSE from MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, the test loss(RMSE) is 3.41 and the training loss(RMSE) is 3.39. Giving a final score of max(0, 100-3.41) = 96.59"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
